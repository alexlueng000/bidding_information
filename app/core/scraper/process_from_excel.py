import asyncio
import json
from typing import Optional, Dict

import pandas as pd
import numpy as np
from openai import OpenAI
from dotenv import load_dotenv
import os

from app.db.mongodb import get_database

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key, base_url="https://openkey.cloud/v1")

# University keyword to collection name mapping
UNIVERSITY_KEYWORDS = {
    "å—æ–¹ç§‘æŠ€å¤§å­¦": "nkd",
    "æ·±åœ³å¤§å­¦": "szu",
    "æ·±åœ³æŠ€æœ¯å¤§å­¦": "sztu",
    "æ·±åœ³å›½é™…é‡å­ç ”ç©¶é™¢": "siqse",
    "æ·±åœ³å…ˆè¿›å…‰æºç ”ç©¶é™¢": "iasf",
    "åŒ—äº¬å¤§å­¦æ·±åœ³ç ”ç©¶ç”Ÿé™¢": "pkusz",
    "æ¸…åå¤§å­¦æ·±åœ³å›½é™…ç ”ç©¶ç”Ÿé™¢": "tsinghua",
    "æ·±åœ³ä¿¡æ¯èŒä¸šæŠ€æœ¯å­¦é™¢": "sziit",
    "æ·±åœ³æ¹¾å®éªŒå®¤": "szbl",
    "æ·±åœ³åŒ—ç†è«æ–¯ç§‘å¤§å­¦": "smbu",
    "åŒ—äº¬ç†å·¥å¤§å­¦æ·±åœ³æ±½è½¦ç ”ç©¶é™¢": "szari",
    "æ·±åœ³åŒ»å­¦ç§‘å­¦é™¢": "szyxkxy",
    "å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ï¼ˆæ·±åœ³ï¼‰": "hgd",
    "é¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼ˆæ·±åœ³ï¼‰": "hkc",
    "æ·±åœ³ç†å·¥å¤§å­¦": "szlg",
    "æ·±åœ³èŒä¸šæŠ€æœ¯å¤§å­¦": "szzyjs",
    "é¹åŸå®éªŒå®¤": "pcsys",
    "ä¸­å±±å¤§å­¦æ·±åœ³æ ¡åŒº": "szust"
}

# Column name mapping: Chinese -> English
COLUMN_MAPPING = {
    'é¡¹ç›®åç§°': 'title',
    'é‡‡è´­å•ä½': 'organization',
    'é¢„ç®—é‡‘é¢': 'budget',
    'é‡‡è´­å“ç›®': 'category',
    'é‡‡è´­éœ€æ±‚æ¦‚å†µ': 'description',
    'é¢„è®¡é‡‡è´­æ—¶é—´': 'expected_time',
    'å‘å¸ƒæ—¶é—´': 'publish_date',
    'å¤‡æ³¨': 'remarks'
}


def filter_after_date(df: pd.DataFrame, date_str: str, date_column: str = 'å‘å¸ƒæ—¶é—´') -> pd.DataFrame:
    """Filter rows where date_column is after the given date."""
    df_copy = df.copy()
    df_copy[date_column] = pd.to_datetime(df_copy[date_column], errors='coerce')
    target_date = pd.to_datetime(date_str)
    return df_copy[df_copy[date_column] > target_date]


def classify_university(title: str, organization: str) -> Optional[str]:
    """
    Classify which university this project belongs to based on title and organization.
    Returns the collection name or None.
    """
    for keyword, collection_name in UNIVERSITY_KEYWORDS.items():
        if keyword in title or keyword in organization:
            # Special handling for æ·±åœ³å¤§å­¦
            if keyword == "æ·±åœ³å¤§å­¦":
                if 'æ€»åŒ»é™¢' not in title and 'é™„å±ä¸­å­¦' not in title and 'é™„å±åå—åŒ»é™¢' not in title:
                    return collection_name
            # Special handling for å—æ–¹ç§‘æŠ€å¤§å­¦
            elif keyword == "å—æ–¹ç§‘æŠ€å¤§å­¦":
                if 'å—æ–¹ç§‘æŠ€å¤§å­¦åŒ»é™¢' != organization:
                    return collection_name
            else:
                return collection_name
    return None


def row_to_dict_with_mapping(row: pd.Series) -> Dict[str, any]:
    """Convert pandas Series to dict, mapping Chinese column names to English."""
    result = {}
    for key, value in row.items():
        if pd.isna(value):
            mapped_value = None
        elif isinstance(value, pd.Timestamp):
            mapped_value = str(value)
        elif isinstance(value, (np.integer, np.floating)):
            mapped_value = int(value) if isinstance(value, np.integer) else float(value)
        else:
            mapped_value = value

        # Map Chinese column names to English
        english_key = COLUMN_MAPPING.get(key, key)
        result[english_key] = mapped_value
    return result


async def insert_info_to_db_from_excel(row_data: Dict[str, any]) -> bool:
    """
    Insert bidding info from Excel into database using the same logic as web scraper.
    Stores data with English field names.

    Args:
        row_data: Dictionary containing all column values from Excel row (with English keys)

    Returns:
        bool: True if inserted, False if skipped (duplicate)
    """
    import datetime

    db = await get_database()

    # Extract key fields for duplicate check
    title = row_data.get('title', '')
    publish_date = row_data.get('publish_date', '')

    # Check if already exists (by title + publish_date)
    query = {
        "title": title,
        "publish_date": publish_date
    }
    exist_info = await db.bidding_infomation.find_one(query)
    if exist_info:
        print(f"[insert_from_excel] â­ï¸  Already exists: {title}")
        return False

    # Add metadata fields
    row_data['created_at'] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Ensure publish_date is string format
    if isinstance(row_data.get('publish_date'), pd.Timestamp):
        row_data['publish_date'] = row_data['publish_date'].strftime('%Y-%m-%d')

    # Insert into main table
    await db.bidding_infomation.insert_one(row_data)
    print(f"[insert_from_excel] âœ… Inserted to main table: {title}")

    # Classify by university
    organization = row_data.get('organization', '')
    collection_name = classify_university(title, organization)
    if not collection_name:
        print(f"[insert_from_excel] â­ï¸  No university match, skipping: {title}")
        return True

    # Use OpenAI to determine if it's a "goods" (è´§ç‰©ç±») project
    project_name = row_data.get('title', '')
    category = row_data.get('category', '')
    description = row_data.get('description', '')

    try:
        completion = client.chat.completions.create(
            model="gpt-4o-2024-08-06",
            stream=False,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æ‹›æ ‡é‡‡è´­ä¸“å®¶ï¼Œè¯·æ ¹æ®æˆ‘æä¾›çš„æ‹›æ ‡é¡¹ç›®æè¿°ä¿¡æ¯ï¼Œåˆ¤æ–­è¿™ä¸ªé¡¹ç›®æ˜¯å¦å±äºè´§ç‰©ç±»é¡¹ç›®ã€‚"},
                {"role": "user", "content": f"ä»¥ä¸‹æ˜¯é¡¹ç›®æè¿°ï¼š\n"
                                f"é¡¹ç›®åç§°ï¼š{project_name}\n"
                                f"é‡‡è´­å“ç›®ï¼š{category}\n"
                                f"é‡‡è´­éœ€æ±‚æ¦‚å†µï¼š{description}\n"
                                "è¯·ç”¨JSONæ ¼å¼å›å¤'true'æˆ–è€…'false':{is_good: ...}"}
            ],
            response_format={"type": "json_object"}
        )

        response_data = json.loads(completion.choices[0].message.content)
        is_good = response_data.get("is_good", False)
    except (json.JSONDecodeError, Exception) as e:
        print(f"[insert_from_excel] âš ï¸  OpenAI API error: {e}")
        is_good = False

    # Add is_good field
    row_data['is_good'] = is_good

    # Insert into university-specific collection
    collection = db[collection_name]
    await collection.insert_one(row_data)
    print(f"[insert_from_excel] âœ… Inserted to {collection_name}: {title} (is_good={is_good})")

    return True


async def process_from_excel(
    file_path: str = 'projects.xlsx',
    sheet_name: str = 'é‡‡è´­æ„å‘é¡¹ç›®åˆ—è¡¨',
    skiprows: int = 1,
    after_date: str = '2025-5-31',
    date_column: str = 'å‘å¸ƒæ—¶é—´'
):
    """
    Process bidding information from local Excel file and insert into database.
    Stores data with English field names.

    Args:
        file_path: Path to the Excel file
        sheet_name: Sheet name in the Excel file
        skiprows: Number of rows to skip from the top
        after_date: Only process records where date_column is after this date
        date_column: Column name to use for date filtering
    """
    try:
        df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=skiprows)
        print(f"[process_from_excel] ğŸ“– Loaded {len(df)} rows from {file_path}")
    except FileNotFoundError:
        print(f"[process_from_excel] âŒ File not found: {file_path}")
        return
    except Exception as e:
        print(f"[process_from_excel] âŒ Error reading Excel: {e}")
        return

    print(f"[process_from_excel] ğŸ“‹ Columns found: {list(df.columns)}")

    # Filter by date
    if date_column in df.columns:
        filtered_df = filter_after_date(df, after_date, date_column)
        print(f"[process_from_excel] ğŸ“… Filtered to {len(filtered_df)} rows after {after_date}")
    else:
        print(f"[process_from_excel] âš ï¸  Date column '{date_column}' not found, using all rows")
        filtered_df = df.copy()

    # Process each row
    success_count = 0
    skip_count = 0

    for index, row in filtered_df.iterrows():
        # Convert row to dict with English field names
        row_data = row_to_dict_with_mapping(row)

        inserted = await insert_info_to_db_from_excel(row_data)
        if inserted:
            success_count += 1
        else:
            skip_count += 1

    print(f"[process_from_excel] âœ¨ Complete! Inserted: {success_count}, Skipped (duplicates): {skip_count}")


async def main():
    """Example usage."""
    await process_from_excel(
        file_path='projects.xlsx',
        sheet_name='é‡‡è´­æ„å‘é¡¹ç›®åˆ—è¡¨',
        skiprows=1,
        after_date='2025-5-31',
        date_column='å‘å¸ƒæ—¶é—´'
    )


if __name__ == "__main__":
    asyncio.run(main())
